{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Tensorflow dependencies ###\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "### Some constants ###\n",
    "lr = 1e-4\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "img_shape = (256, 256, 3)\n",
    "latent_dim = 100\n",
    "\n",
    "### Discriminators and Generators weights path ###\n",
    "g_weights_path = \"weights/g.weights.hdf5\" # G : X -> Y\n",
    "f_weights_path = \"weights/f.weights.hdf5\" # F : Y -> X\n",
    "x_weights_path = \"weights/x.weights.hdf5\" # discriminate X and F(Y)\n",
    "y_weights_path = \"weights/y.weights.hdf5\" # discriminate Y and G(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
    "                              with_info=True, as_supervised=True)\n",
    "\n",
    "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
    "test_horses, test_zebras = dataset['testA'], dataset['testB'] \n",
    "\n",
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(image, size=[img_shape[0], img_shape[1], 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 112 x 112 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_image_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image, label):\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "train_horses = train_horses.map(\n",
    "    preprocess_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "train_zebras = train_zebras.map(\n",
    "    preprocess_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "test_horses = test_horses.map(\n",
    "    preprocess_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "test_zebras = test_zebras.map(\n",
    "    preprocess_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generator G and F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_36 (Sequential)      (None, None, None, 6 3072        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_37 (Sequential)      (None, None, None, 1 131328      sequential_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_38 (Sequential)      (None, None, None, 2 524800      sequential_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_39 (Sequential)      (None, None, None, 5 2098176     sequential_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_40 (Sequential)      (None, None, None, 5 4195328     sequential_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_41 (Sequential)      (None, None, None, 5 4195328     sequential_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_42 (Sequential)      (None, None, None, 5 4195328     sequential_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_43 (Sequential)      (None, None, None, 5 4195328     sequential_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_44 (Sequential)      (None, None, None, 5 4195328     sequential_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     multiple             0           sequential_44[0][0]              \n",
      "                                                                 sequential_42[0][0]              \n",
      "                                                                 sequential_45[0][0]              \n",
      "                                                                 sequential_41[0][0]              \n",
      "                                                                 sequential_46[0][0]              \n",
      "                                                                 sequential_40[0][0]              \n",
      "                                                                 sequential_47[0][0]              \n",
      "                                                                 sequential_39[0][0]              \n",
      "                                                                 sequential_48[0][0]              \n",
      "                                                                 sequential_38[0][0]              \n",
      "                                                                 sequential_49[0][0]              \n",
      "                                                                 sequential_37[0][0]              \n",
      "                                                                 sequential_50[0][0]              \n",
      "                                                                 sequential_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_45 (Sequential)      (None, None, None, 5 8389632     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_46 (Sequential)      (None, None, None, 5 8389632     concatenate_2[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_47 (Sequential)      (None, None, None, 5 8389632     concatenate_2[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_48 (Sequential)      (None, None, None, 2 4194816     concatenate_2[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_49 (Sequential)      (None, None, None, 1 1048832     concatenate_2[4][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_50 (Sequential)      (None, None, None, 6 262272      concatenate_2[5][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DTran (None, None, None, 3 6147        concatenate_2[6][0]              \n",
      "==================================================================================================\n",
      "Total params: 54,414,979\n",
      "Trainable params: 54,414,979\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_51 (Sequential)      (None, None, None, 6 3072        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_52 (Sequential)      (None, None, None, 1 131328      sequential_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_53 (Sequential)      (None, None, None, 2 524800      sequential_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_54 (Sequential)      (None, None, None, 5 2098176     sequential_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_55 (Sequential)      (None, None, None, 5 4195328     sequential_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_56 (Sequential)      (None, None, None, 5 4195328     sequential_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_57 (Sequential)      (None, None, None, 5 4195328     sequential_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_58 (Sequential)      (None, None, None, 5 4195328     sequential_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_59 (Sequential)      (None, None, None, 5 4195328     sequential_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     multiple             0           sequential_59[0][0]              \n",
      "                                                                 sequential_57[0][0]              \n",
      "                                                                 sequential_60[0][0]              \n",
      "                                                                 sequential_56[0][0]              \n",
      "                                                                 sequential_61[0][0]              \n",
      "                                                                 sequential_55[0][0]              \n",
      "                                                                 sequential_62[0][0]              \n",
      "                                                                 sequential_54[0][0]              \n",
      "                                                                 sequential_63[0][0]              \n",
      "                                                                 sequential_53[0][0]              \n",
      "                                                                 sequential_64[0][0]              \n",
      "                                                                 sequential_52[0][0]              \n",
      "                                                                 sequential_65[0][0]              \n",
      "                                                                 sequential_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_60 (Sequential)      (None, None, None, 5 8389632     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_61 (Sequential)      (None, None, None, 5 8389632     concatenate_3[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_62 (Sequential)      (None, None, None, 5 8389632     concatenate_3[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_63 (Sequential)      (None, None, None, 2 4194816     concatenate_3[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_64 (Sequential)      (None, None, None, 1 1048832     concatenate_3[4][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_65 (Sequential)      (None, None, None, 6 262272      concatenate_3[5][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_31 (Conv2DTran (None, None, None, 3 6147        concatenate_3[6][0]              \n",
      "==================================================================================================\n",
      "Total params: 54,414,979\n",
      "Trainable params: 54,414,979\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# def make_generator(name):\n",
    "#     inputs = Input(shape=(latent_dim,))\n",
    "    \n",
    "#     x = Dense(7*7*256, use_bias=False)(inputs)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "#     x = Reshape(target_shape=(7,7,256))(x)\n",
    "    \n",
    "#     # Size = 128 x 14 x 14 \n",
    "#     x = Conv2DTranspose(128, kernel_size=(4,4), strides=(2,2), padding='same',\n",
    "#                         use_bias=False)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "#     # Size = 64 x 28 x 28\n",
    "#     x = Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same',\n",
    "#                        use_bias=False)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "#     # Size = 32 x 56 x 56\n",
    "#     x = Conv2DTranspose(32, kernel_size=(4,4), strides=(2,2), padding='same',\n",
    "#                        use_bias=False)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "#     # Size = 16 x 112 x 112\n",
    "#     x = Conv2DTranspose(16, kernel_size=(4,4), strides=(2,2), padding='same',\n",
    "#                        use_bias=False)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "#     # Use tanh so that it is [-1, 1]\n",
    "#     x = Conv2D(img_shape[-1], kernel_size=(4,4), padding='same', use_bias=False, activation='tanh')(x)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=x, name=f'Generator_{name}')\n",
    "#     return model\n",
    "\n",
    "G = pix2pix.unet_generator(img_shape[-1], norm_type='instancenorm') # make_generator('G')\n",
    "F = pix2pix.unet_generator(img_shape[-1], norm_type='instancenorm') # make_generator('F')\n",
    "\n",
    "print(G.summary())\n",
    "print(F.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "sequential_66 (Sequential)   (None, None, None, 64)    3072      \n",
      "_________________________________________________________________\n",
      "sequential_67 (Sequential)   (None, None, None, 128)   131328    \n",
      "_________________________________________________________________\n",
      "sequential_68 (Sequential)   (None, None, None, 256)   524800    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, None, None, 512)   2097152   \n",
      "_________________________________________________________________\n",
      "instance_normalization_64 (I (None, None, None, 512)   1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, None, None, 1)     8193      \n",
      "=================================================================\n",
      "Total params: 2,765,569\n",
      "Trainable params: 2,765,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "sequential_69 (Sequential)   (None, None, None, 64)    3072      \n",
      "_________________________________________________________________\n",
      "sequential_70 (Sequential)   (None, None, None, 128)   131328    \n",
      "_________________________________________________________________\n",
      "sequential_71 (Sequential)   (None, None, None, 256)   524800    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, None, None, 512)   2097152   \n",
      "_________________________________________________________________\n",
      "instance_normalization_67 (I (None, None, None, 512)   1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, None, None, 1)     8193      \n",
      "=================================================================\n",
      "Total params: 2,765,569\n",
      "Trainable params: 2,765,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# def make_discriminator(name):\n",
    "#     inputs = Input(shape=img_shape)\n",
    "    \n",
    "#     x = Conv2D(16, kernel_size=(4,4), strides=(2,2), padding='same')(inputs)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "#     x = Conv2D(32, kernel_size=(4,4), strides=(2,2), padding='same')(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "#     x = Conv2D(64, kernel_size=(4,4), strides=(2,2), padding='same')(x)\n",
    "#     x = LeakyReLU()(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "#     x = Conv2D(128, kernel_size=(4,4), strides=(2,2), padding='same')(x)\n",
    "#     x = LeakyReLU(alpha=0.2)(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "    \n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(1)(x)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=x, name=f'Discriminator_{name}')\n",
    "#     return model\n",
    "\n",
    "D_x = pix2pix.discriminator(norm_type='instancenorm', target=False) # make_discriminator(\"X\")\n",
    "D_y = pix2pix.discriminator(norm_type='instancenorm', target=False) # make_discriminator(\"Y\")\n",
    "\n",
    "print(D_x.summary())\n",
    "print(D_y.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = BinaryCrossentropy(from_logits=True)\n",
    "LAMBDA = 10.0 # scale factor for cycle consistency and identity loss functions\n",
    "\n",
    "def generator_loss(D_fake):\n",
    "    ones = tf.ones_like(D_fake, dtype=tf.float32)\n",
    "    loss = bce(ones, D_fake)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def discriminator_loss(D_real, D_fake):\n",
    "    ones = tf.ones_like(D_real, dtype=tf.float32)\n",
    "    zeros = tf.zeros_like(D_fake, dtype=tf.float32)\n",
    "    \n",
    "    real_loss = bce(ones, D_real)\n",
    "    fake_loss = bce(zeros, D_fake)\n",
    "    loss = real_loss + fake_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def cycle_consistency_loss(cycled_images, real_images):\n",
    "    return LAMBDA * tf.reduce_mean(tf.abs(cycled_images - real_images))\n",
    "\n",
    "def identity_loss(same_images, real_images):\n",
    "    return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(same_images - real_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Batch #1 | Epoch #1, G_loss = 8.28, F_loss = 7.35, X_loss = 1.37, Y_loss = 1.49\n",
      "[INFO] Batch #2 | Epoch #1, G_loss = 9.93, F_loss = 10.43, X_loss = 1.97, Y_loss = 1.80\n",
      "[INFO] Batch #3 | Epoch #1, G_loss = 12.37, F_loss = 12.73, X_loss = 1.71, Y_loss = 1.51\n",
      "[INFO] Batch #4 | Epoch #1, G_loss = 14.87, F_loss = 15.31, X_loss = 1.31, Y_loss = 1.57\n",
      "[INFO] Batch #5 | Epoch #1, G_loss = 11.74, F_loss = 11.67, X_loss = 1.51, Y_loss = 1.40\n",
      "[INFO] Batch #6 | Epoch #1, G_loss = 7.81, F_loss = 8.15, X_loss = 1.44, Y_loss = 1.43\n",
      "[INFO] Batch #7 | Epoch #1, G_loss = 11.55, F_loss = 11.71, X_loss = 1.27, Y_loss = 1.31\n",
      "[INFO] Batch #8 | Epoch #1, G_loss = 11.01, F_loss = 10.94, X_loss = 1.46, Y_loss = 1.45\n",
      "[INFO] Batch #9 | Epoch #1, G_loss = 8.91, F_loss = 8.36, X_loss = 1.36, Y_loss = 1.53\n",
      "[INFO] Batch #10 | Epoch #1, G_loss = 7.34, F_loss = 7.22, X_loss = 1.51, Y_loss = 1.49\n",
      "[INFO] Batch #11 | Epoch #1, G_loss = 10.93, F_loss = 11.39, X_loss = 1.35, Y_loss = 1.27\n",
      "[INFO] Batch #12 | Epoch #1, G_loss = 8.00, F_loss = 8.43, X_loss = 1.38, Y_loss = 1.41\n",
      "[INFO] Batch #13 | Epoch #1, G_loss = 16.13, F_loss = 15.55, X_loss = 1.23, Y_loss = 1.30\n",
      "[INFO] Batch #14 | Epoch #1, G_loss = 8.91, F_loss = 8.11, X_loss = 1.55, Y_loss = 1.46\n",
      "[INFO] Batch #15 | Epoch #1, G_loss = 9.28, F_loss = 9.31, X_loss = 1.18, Y_loss = 1.32\n",
      "[INFO] Batch #16 | Epoch #1, G_loss = 12.80, F_loss = 12.54, X_loss = 1.32, Y_loss = 1.43\n",
      "[INFO] Batch #17 | Epoch #1, G_loss = 9.62, F_loss = 8.78, X_loss = 1.46, Y_loss = 1.34\n",
      "[INFO] Batch #18 | Epoch #1, G_loss = 16.55, F_loss = 16.29, X_loss = 1.05, Y_loss = 1.29\n",
      "[INFO] Batch #19 | Epoch #1, G_loss = 12.66, F_loss = 12.80, X_loss = 1.40, Y_loss = 1.50\n",
      "[INFO] Batch #20 | Epoch #1, G_loss = 13.30, F_loss = 13.18, X_loss = 1.28, Y_loss = 1.51\n",
      "[INFO] Batch #21 | Epoch #1, G_loss = 12.44, F_loss = 11.37, X_loss = 1.20, Y_loss = 1.27\n",
      "[INFO] Batch #22 | Epoch #1, G_loss = 9.24, F_loss = 9.74, X_loss = 1.61, Y_loss = 1.48\n",
      "[INFO] Batch #23 | Epoch #1, G_loss = 7.41, F_loss = 7.66, X_loss = 1.54, Y_loss = 1.33\n",
      "[INFO] Batch #24 | Epoch #1, G_loss = 8.41, F_loss = 8.31, X_loss = 1.47, Y_loss = 1.43\n",
      "[INFO] Batch #25 | Epoch #1, G_loss = 8.84, F_loss = 8.59, X_loss = 1.27, Y_loss = 1.33\n",
      "[INFO] Batch #26 | Epoch #1, G_loss = 12.17, F_loss = 10.78, X_loss = 1.63, Y_loss = 1.35\n",
      "[INFO] Batch #27 | Epoch #1, G_loss = 11.62, F_loss = 11.07, X_loss = 1.68, Y_loss = 1.62\n",
      "[INFO] Batch #28 | Epoch #1, G_loss = 9.87, F_loss = 10.31, X_loss = 1.53, Y_loss = 1.42\n",
      "[INFO] Batch #29 | Epoch #1, G_loss = 10.66, F_loss = 10.18, X_loss = 1.28, Y_loss = 1.34\n",
      "[INFO] Batch #30 | Epoch #1, G_loss = 10.38, F_loss = 9.66, X_loss = 1.32, Y_loss = 1.39\n",
      "[INFO] Batch #31 | Epoch #1, G_loss = 13.67, F_loss = 13.15, X_loss = 1.22, Y_loss = 1.26\n",
      "[INFO] Batch #32 | Epoch #1, G_loss = 9.86, F_loss = 10.13, X_loss = 1.28, Y_loss = 1.27\n",
      "[INFO] Batch #33 | Epoch #1, G_loss = 12.44, F_loss = 13.09, X_loss = 1.34, Y_loss = 1.38\n",
      "[INFO] Batch #34 | Epoch #1, G_loss = 17.21, F_loss = 18.55, X_loss = 1.16, Y_loss = 1.32\n",
      "[INFO] Batch #35 | Epoch #1, G_loss = 13.68, F_loss = 12.49, X_loss = 1.54, Y_loss = 1.47\n",
      "[INFO] Batch #36 | Epoch #1, G_loss = 7.60, F_loss = 7.11, X_loss = 1.41, Y_loss = 1.39\n",
      "[INFO] Batch #37 | Epoch #1, G_loss = 9.89, F_loss = 9.27, X_loss = 1.46, Y_loss = 1.42\n",
      "[INFO] Batch #38 | Epoch #1, G_loss = 11.92, F_loss = 12.09, X_loss = 1.31, Y_loss = 1.31\n",
      "[INFO] Batch #39 | Epoch #1, G_loss = 11.45, F_loss = 11.03, X_loss = 1.47, Y_loss = 1.36\n",
      "[INFO] Batch #40 | Epoch #1, G_loss = 8.31, F_loss = 8.32, X_loss = 1.43, Y_loss = 1.37\n",
      "[INFO] Batch #41 | Epoch #1, G_loss = 12.19, F_loss = 11.66, X_loss = 1.22, Y_loss = 1.39\n",
      "[INFO] Batch #42 | Epoch #1, G_loss = 8.00, F_loss = 8.09, X_loss = 1.24, Y_loss = 1.48\n",
      "[INFO] Batch #43 | Epoch #1, G_loss = 11.92, F_loss = 10.79, X_loss = 1.45, Y_loss = 1.37\n",
      "[INFO] Batch #44 | Epoch #1, G_loss = 7.48, F_loss = 7.26, X_loss = 1.05, Y_loss = 1.46\n",
      "[INFO] Batch #45 | Epoch #1, G_loss = 8.77, F_loss = 8.78, X_loss = 1.33, Y_loss = 1.35\n",
      "[INFO] Batch #46 | Epoch #1, G_loss = 6.62, F_loss = 6.62, X_loss = 1.50, Y_loss = 1.42\n",
      "[INFO] Batch #47 | Epoch #1, G_loss = 12.36, F_loss = 11.70, X_loss = 1.24, Y_loss = 1.38\n",
      "[INFO] Batch #48 | Epoch #1, G_loss = 10.36, F_loss = 10.06, X_loss = 1.43, Y_loss = 1.24\n",
      "[INFO] Batch #49 | Epoch #1, G_loss = 10.81, F_loss = 10.71, X_loss = 1.46, Y_loss = 1.53\n",
      "[INFO] Batch #50 | Epoch #1, G_loss = 11.68, F_loss = 11.72, X_loss = 1.16, Y_loss = 1.38\n",
      "[INFO] Batch #51 | Epoch #1, G_loss = 9.29, F_loss = 8.47, X_loss = 1.47, Y_loss = 1.35\n",
      "[INFO] Batch #52 | Epoch #1, G_loss = 9.83, F_loss = 9.91, X_loss = 1.77, Y_loss = 1.64\n",
      "[INFO] Batch #53 | Epoch #1, G_loss = 12.80, F_loss = 12.01, X_loss = 1.25, Y_loss = 1.35\n",
      "[INFO] Batch #54 | Epoch #1, G_loss = 12.98, F_loss = 12.48, X_loss = 1.19, Y_loss = 1.38\n",
      "[INFO] Batch #55 | Epoch #1, G_loss = 10.01, F_loss = 9.62, X_loss = 1.22, Y_loss = 1.25\n",
      "[INFO] Batch #56 | Epoch #1, G_loss = 8.46, F_loss = 8.25, X_loss = 1.39, Y_loss = 1.48\n",
      "[INFO] Batch #57 | Epoch #1, G_loss = 9.43, F_loss = 9.18, X_loss = 1.61, Y_loss = 1.42\n",
      "[INFO] Batch #58 | Epoch #1, G_loss = 11.45, F_loss = 11.35, X_loss = 1.09, Y_loss = 1.30\n",
      "[INFO] Batch #59 | Epoch #1, G_loss = 7.31, F_loss = 7.12, X_loss = 1.55, Y_loss = 1.46\n",
      "[INFO] Batch #60 | Epoch #1, G_loss = 13.45, F_loss = 13.32, X_loss = 1.27, Y_loss = 1.32\n",
      "[INFO] Batch #61 | Epoch #1, G_loss = 14.61, F_loss = 13.05, X_loss = 1.25, Y_loss = 1.31\n",
      "[INFO] Batch #62 | Epoch #1, G_loss = 12.14, F_loss = 11.36, X_loss = 0.98, Y_loss = 1.23\n",
      "[INFO] Batch #63 | Epoch #1, G_loss = 9.74, F_loss = 9.55, X_loss = 1.37, Y_loss = 1.33\n",
      "[INFO] Batch #64 | Epoch #1, G_loss = 7.37, F_loss = 7.44, X_loss = 1.36, Y_loss = 1.23\n",
      "[INFO] Batch #65 | Epoch #1, G_loss = 10.22, F_loss = 9.87, X_loss = 1.64, Y_loss = 1.35\n",
      "[INFO] Batch #66 | Epoch #1, G_loss = 12.91, F_loss = 12.75, X_loss = 1.18, Y_loss = 1.17\n",
      "[INFO] Batch #67 | Epoch #1, G_loss = 8.22, F_loss = 7.77, X_loss = 1.34, Y_loss = 1.22\n",
      "[INFO] Batch #68 | Epoch #1, G_loss = 11.79, F_loss = 11.48, X_loss = 1.40, Y_loss = 1.30\n",
      "[INFO] Batch #69 | Epoch #1, G_loss = 16.25, F_loss = 16.66, X_loss = 1.10, Y_loss = 1.43\n",
      "[INFO] Batch #70 | Epoch #1, G_loss = 8.10, F_loss = 8.19, X_loss = 1.39, Y_loss = 1.55\n",
      "[INFO] Batch #71 | Epoch #1, G_loss = 9.00, F_loss = 8.81, X_loss = 1.75, Y_loss = 1.37\n",
      "[INFO] Batch #72 | Epoch #1, G_loss = 15.28, F_loss = 15.27, X_loss = 1.20, Y_loss = 1.55\n",
      "[INFO] Batch #73 | Epoch #1, G_loss = 12.84, F_loss = 11.71, X_loss = 1.50, Y_loss = 1.52\n",
      "[INFO] Batch #74 | Epoch #1, G_loss = 12.17, F_loss = 12.38, X_loss = 1.53, Y_loss = 1.58\n",
      "[INFO] Batch #75 | Epoch #1, G_loss = 9.20, F_loss = 9.24, X_loss = 1.50, Y_loss = 1.58\n",
      "[INFO] Batch #76 | Epoch #1, G_loss = 13.84, F_loss = 13.93, X_loss = 1.23, Y_loss = 1.43\n",
      "[INFO] Batch #77 | Epoch #1, G_loss = 12.46, F_loss = 12.20, X_loss = 1.58, Y_loss = 1.45\n",
      "[INFO] Batch #78 | Epoch #1, G_loss = 12.86, F_loss = 12.45, X_loss = 1.47, Y_loss = 1.45\n",
      "[INFO] Batch #79 | Epoch #1, G_loss = 11.87, F_loss = 11.43, X_loss = 1.20, Y_loss = 1.42\n",
      "[INFO] Batch #80 | Epoch #1, G_loss = 12.20, F_loss = 13.08, X_loss = 1.36, Y_loss = 1.44\n",
      "[INFO] Batch #81 | Epoch #1, G_loss = 10.77, F_loss = 10.67, X_loss = 1.12, Y_loss = 1.50\n",
      "[INFO] Batch #82 | Epoch #1, G_loss = 9.92, F_loss = 10.11, X_loss = 1.47, Y_loss = 1.40\n",
      "[INFO] Batch #83 | Epoch #1, G_loss = 11.25, F_loss = 10.67, X_loss = 1.50, Y_loss = 1.27\n",
      "[INFO] Batch #84 | Epoch #1, G_loss = 10.32, F_loss = 9.88, X_loss = 1.42, Y_loss = 1.30\n",
      "[INFO] Batch #85 | Epoch #1, G_loss = 7.84, F_loss = 7.82, X_loss = 1.29, Y_loss = 1.23\n",
      "[INFO] Batch #86 | Epoch #1, G_loss = 10.88, F_loss = 10.67, X_loss = 1.36, Y_loss = 1.44\n",
      "[INFO] Batch #87 | Epoch #1, G_loss = 11.41, F_loss = 11.82, X_loss = 1.31, Y_loss = 1.29\n",
      "[INFO] Batch #88 | Epoch #1, G_loss = 11.15, F_loss = 11.69, X_loss = 1.37, Y_loss = 1.22\n",
      "[INFO] Batch #89 | Epoch #1, G_loss = 11.58, F_loss = 10.70, X_loss = 1.55, Y_loss = 1.38\n",
      "[INFO] Batch #90 | Epoch #1, G_loss = 9.08, F_loss = 8.82, X_loss = 1.19, Y_loss = 1.30\n",
      "[INFO] Batch #91 | Epoch #1, G_loss = 10.08, F_loss = 10.25, X_loss = 1.38, Y_loss = 1.22\n",
      "[INFO] Batch #92 | Epoch #1, G_loss = 12.76, F_loss = 12.26, X_loss = 1.19, Y_loss = 1.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Batch #93 | Epoch #1, G_loss = 10.58, F_loss = 10.32, X_loss = 1.28, Y_loss = 1.48\n",
      "[INFO] Batch #94 | Epoch #1, G_loss = 8.49, F_loss = 8.63, X_loss = 1.49, Y_loss = 1.37\n",
      "[INFO] Batch #95 | Epoch #1, G_loss = 12.28, F_loss = 12.46, X_loss = 1.10, Y_loss = 1.46\n",
      "[INFO] Batch #96 | Epoch #1, G_loss = 14.14, F_loss = 13.78, X_loss = 1.47, Y_loss = 1.36\n",
      "[INFO] Batch #97 | Epoch #1, G_loss = 12.09, F_loss = 12.15, X_loss = 1.12, Y_loss = 1.32\n",
      "[INFO] Batch #98 | Epoch #1, G_loss = 11.84, F_loss = 11.03, X_loss = 1.43, Y_loss = 1.41\n",
      "[INFO] Batch #99 | Epoch #1, G_loss = 11.16, F_loss = 10.61, X_loss = 1.24, Y_loss = 1.29\n",
      "[INFO] Batch #100 | Epoch #1, G_loss = 12.13, F_loss = 11.33, X_loss = 1.02, Y_loss = 1.27\n",
      "[INFO] Batch #101 | Epoch #1, G_loss = 9.24, F_loss = 8.71, X_loss = 0.93, Y_loss = 1.28\n",
      "[INFO] Batch #102 | Epoch #1, G_loss = 9.68, F_loss = 9.38, X_loss = 0.99, Y_loss = 1.23\n",
      "[INFO] Batch #103 | Epoch #1, G_loss = 9.95, F_loss = 10.30, X_loss = 1.42, Y_loss = 1.32\n",
      "[INFO] Batch #104 | Epoch #1, G_loss = 15.55, F_loss = 14.61, X_loss = 1.22, Y_loss = 1.28\n",
      "[INFO] Batch #105 | Epoch #1, G_loss = 10.73, F_loss = 10.79, X_loss = 1.29, Y_loss = 1.32\n",
      "[INFO] Batch #106 | Epoch #1, G_loss = 12.41, F_loss = 12.40, X_loss = 1.30, Y_loss = 1.30\n",
      "[INFO] Batch #107 | Epoch #1, G_loss = 9.43, F_loss = 9.80, X_loss = 1.53, Y_loss = 1.39\n",
      "[INFO] Batch #108 | Epoch #1, G_loss = 12.20, F_loss = 12.08, X_loss = 1.20, Y_loss = 1.25\n",
      "[INFO] Batch #109 | Epoch #1, G_loss = 12.19, F_loss = 11.98, X_loss = 1.02, Y_loss = 1.18\n",
      "[INFO] Batch #110 | Epoch #1, G_loss = 11.49, F_loss = 11.02, X_loss = 1.19, Y_loss = 1.12\n",
      "[INFO] Batch #111 | Epoch #1, G_loss = 12.30, F_loss = 11.75, X_loss = 1.20, Y_loss = 1.05\n",
      "[INFO] Batch #112 | Epoch #1, G_loss = 13.24, F_loss = 11.62, X_loss = 1.90, Y_loss = 1.44\n",
      "[INFO] Batch #113 | Epoch #1, G_loss = 12.06, F_loss = 11.78, X_loss = 1.48, Y_loss = 1.43\n",
      "[INFO] Batch #114 | Epoch #1, G_loss = 7.74, F_loss = 8.06, X_loss = 1.68, Y_loss = 1.64\n",
      "[INFO] Batch #115 | Epoch #1, G_loss = 9.24, F_loss = 9.13, X_loss = 1.48, Y_loss = 1.47\n",
      "[INFO] Batch #116 | Epoch #1, G_loss = 11.27, F_loss = 11.83, X_loss = 1.39, Y_loss = 1.41\n",
      "[INFO] Batch #117 | Epoch #1, G_loss = 12.80, F_loss = 11.23, X_loss = 1.31, Y_loss = 1.64\n",
      "[INFO] Batch #118 | Epoch #1, G_loss = 9.50, F_loss = 9.18, X_loss = 1.42, Y_loss = 1.44\n",
      "[INFO] Batch #119 | Epoch #1, G_loss = 12.56, F_loss = 12.04, X_loss = 1.26, Y_loss = 1.30\n",
      "[INFO] Batch #120 | Epoch #1, G_loss = 9.36, F_loss = 8.76, X_loss = 1.27, Y_loss = 1.51\n",
      "[INFO] Batch #121 | Epoch #1, G_loss = 12.61, F_loss = 12.99, X_loss = 1.43, Y_loss = 1.39\n",
      "[INFO] Batch #122 | Epoch #1, G_loss = 11.47, F_loss = 11.25, X_loss = 1.09, Y_loss = 1.22\n",
      "[INFO] Batch #123 | Epoch #1, G_loss = 12.66, F_loss = 12.88, X_loss = 1.03, Y_loss = 1.28\n",
      "[INFO] Batch #124 | Epoch #1, G_loss = 8.68, F_loss = 8.31, X_loss = 1.30, Y_loss = 1.21\n",
      "[INFO] Batch #125 | Epoch #1, G_loss = 11.53, F_loss = 11.43, X_loss = 0.89, Y_loss = 1.18\n",
      "[INFO] Batch #126 | Epoch #1, G_loss = 11.78, F_loss = 12.30, X_loss = 0.99, Y_loss = 1.37\n",
      "[INFO] Batch #127 | Epoch #1, G_loss = 11.41, F_loss = 11.03, X_loss = 1.42, Y_loss = 1.22\n",
      "[INFO] Batch #128 | Epoch #1, G_loss = 11.71, F_loss = 11.31, X_loss = 1.52, Y_loss = 1.17\n",
      "[INFO] Batch #129 | Epoch #1, G_loss = 9.86, F_loss = 9.84, X_loss = 1.04, Y_loss = 1.09\n",
      "[INFO] Batch #130 | Epoch #1, G_loss = 10.71, F_loss = 10.90, X_loss = 1.40, Y_loss = 1.23\n",
      "[INFO] Batch #131 | Epoch #1, G_loss = 12.59, F_loss = 12.52, X_loss = 1.16, Y_loss = 1.38\n",
      "[INFO] Batch #132 | Epoch #1, G_loss = 14.84, F_loss = 14.26, X_loss = 1.17, Y_loss = 0.97\n",
      "[INFO] Batch #133 | Epoch #1, G_loss = 13.21, F_loss = 12.28, X_loss = 1.70, Y_loss = 1.72\n",
      "[INFO] Batch #134 | Epoch #1, G_loss = 7.44, F_loss = 7.59, X_loss = 1.63, Y_loss = 1.52\n",
      "[INFO] Batch #135 | Epoch #1, G_loss = 8.16, F_loss = 8.52, X_loss = 1.08, Y_loss = 1.34\n",
      "[INFO] Batch #136 | Epoch #1, G_loss = 9.82, F_loss = 9.81, X_loss = 1.23, Y_loss = 1.22\n",
      "[INFO] Batch #137 | Epoch #1, G_loss = 9.74, F_loss = 8.55, X_loss = 1.35, Y_loss = 1.27\n",
      "[INFO] Batch #138 | Epoch #1, G_loss = 9.21, F_loss = 9.26, X_loss = 1.34, Y_loss = 1.18\n",
      "[INFO] Batch #139 | Epoch #1, G_loss = 10.53, F_loss = 10.57, X_loss = 0.71, Y_loss = 1.21\n",
      "[INFO] Batch #140 | Epoch #1, G_loss = 10.47, F_loss = 9.60, X_loss = 1.59, Y_loss = 1.06\n",
      "[INFO] Batch #141 | Epoch #1, G_loss = 9.90, F_loss = 9.68, X_loss = 1.71, Y_loss = 1.31\n",
      "[INFO] Batch #142 | Epoch #1, G_loss = 9.70, F_loss = 9.38, X_loss = 1.02, Y_loss = 1.19\n",
      "[INFO] Batch #143 | Epoch #1, G_loss = 8.88, F_loss = 9.10, X_loss = 1.08, Y_loss = 1.02\n",
      "[INFO] Batch #144 | Epoch #1, G_loss = 10.15, F_loss = 10.21, X_loss = 1.43, Y_loss = 1.18\n",
      "[INFO] Batch #145 | Epoch #1, G_loss = 11.70, F_loss = 11.08, X_loss = 1.22, Y_loss = 1.11\n",
      "[INFO] Batch #146 | Epoch #1, G_loss = 9.57, F_loss = 9.75, X_loss = 0.88, Y_loss = 1.17\n",
      "[INFO] Batch #147 | Epoch #1, G_loss = 10.66, F_loss = 9.91, X_loss = 1.14, Y_loss = 1.08\n",
      "[INFO] Batch #148 | Epoch #1, G_loss = 13.46, F_loss = 12.56, X_loss = 1.33, Y_loss = 1.09\n",
      "[INFO] Batch #149 | Epoch #1, G_loss = 12.50, F_loss = 11.26, X_loss = 1.47, Y_loss = 1.02\n",
      "[INFO] Batch #150 | Epoch #1, G_loss = 10.81, F_loss = 11.48, X_loss = 1.77, Y_loss = 1.12\n",
      "[INFO] Batch #151 | Epoch #1, G_loss = 10.60, F_loss = 11.57, X_loss = 1.67, Y_loss = 1.48\n",
      "[INFO] Batch #152 | Epoch #1, G_loss = 8.38, F_loss = 8.19, X_loss = 1.51, Y_loss = 1.41\n",
      "[INFO] Batch #153 | Epoch #1, G_loss = 12.08, F_loss = 11.80, X_loss = 1.42, Y_loss = 1.31\n",
      "[INFO] Batch #154 | Epoch #1, G_loss = 8.15, F_loss = 8.10, X_loss = 1.38, Y_loss = 1.41\n",
      "[INFO] Batch #155 | Epoch #1, G_loss = 12.50, F_loss = 12.18, X_loss = 1.25, Y_loss = 1.26\n",
      "[INFO] Batch #156 | Epoch #1, G_loss = 11.81, F_loss = 11.38, X_loss = 1.36, Y_loss = 1.43\n",
      "[INFO] Batch #157 | Epoch #1, G_loss = 8.85, F_loss = 8.54, X_loss = 1.31, Y_loss = 1.43\n",
      "[INFO] Batch #158 | Epoch #1, G_loss = 13.79, F_loss = 13.83, X_loss = 1.27, Y_loss = 1.56\n",
      "[INFO] Batch #159 | Epoch #1, G_loss = 9.31, F_loss = 9.54, X_loss = 1.23, Y_loss = 1.47\n",
      "[INFO] Batch #160 | Epoch #1, G_loss = 8.58, F_loss = 8.54, X_loss = 0.97, Y_loss = 1.59\n",
      "[INFO] Batch #161 | Epoch #1, G_loss = 9.79, F_loss = 8.99, X_loss = 1.17, Y_loss = 1.40\n",
      "[INFO] Batch #162 | Epoch #1, G_loss = 9.83, F_loss = 9.01, X_loss = 1.22, Y_loss = 1.35\n",
      "[INFO] Batch #163 | Epoch #1, G_loss = 8.95, F_loss = 9.07, X_loss = 1.47, Y_loss = 1.44\n",
      "[INFO] Batch #164 | Epoch #1, G_loss = 12.82, F_loss = 12.68, X_loss = 1.09, Y_loss = 1.43\n",
      "[INFO] Batch #165 | Epoch #1, G_loss = 11.38, F_loss = 11.65, X_loss = 0.85, Y_loss = 1.11\n",
      "[INFO] Batch #166 | Epoch #1, G_loss = 11.83, F_loss = 12.00, X_loss = 0.71, Y_loss = 1.11\n",
      "[INFO] Batch #167 | Epoch #1, G_loss = 10.16, F_loss = 9.76, X_loss = 1.13, Y_loss = 1.41\n",
      "[INFO] Batch #168 | Epoch #1, G_loss = 9.73, F_loss = 9.30, X_loss = 0.92, Y_loss = 1.10\n",
      "[INFO] Batch #169 | Epoch #1, G_loss = 13.30, F_loss = 13.48, X_loss = 0.97, Y_loss = 0.99\n",
      "[INFO] Batch #170 | Epoch #1, G_loss = 7.88, F_loss = 7.68, X_loss = 0.94, Y_loss = 1.16\n",
      "[INFO] Batch #171 | Epoch #1, G_loss = 10.25, F_loss = 9.67, X_loss = 1.67, Y_loss = 1.46\n",
      "[INFO] Batch #172 | Epoch #1, G_loss = 13.06, F_loss = 12.97, X_loss = 1.37, Y_loss = 1.25\n",
      "[INFO] Batch #173 | Epoch #1, G_loss = 11.25, F_loss = 10.52, X_loss = 1.46, Y_loss = 1.09\n",
      "[INFO] Batch #174 | Epoch #1, G_loss = 13.10, F_loss = 13.75, X_loss = 1.97, Y_loss = 2.04\n",
      "[INFO] Batch #175 | Epoch #1, G_loss = 13.35, F_loss = 12.64, X_loss = 1.49, Y_loss = 1.51\n",
      "[INFO] Batch #176 | Epoch #1, G_loss = 10.43, F_loss = 11.17, X_loss = 1.46, Y_loss = 1.54\n",
      "[INFO] Batch #177 | Epoch #1, G_loss = 11.86, F_loss = 12.26, X_loss = 1.40, Y_loss = 1.61\n",
      "[INFO] Batch #178 | Epoch #1, G_loss = 9.28, F_loss = 8.70, X_loss = 1.44, Y_loss = 1.59\n",
      "[INFO] Batch #179 | Epoch #1, G_loss = 10.61, F_loss = 11.46, X_loss = 1.50, Y_loss = 1.73\n",
      "[INFO] Batch #180 | Epoch #1, G_loss = 11.01, F_loss = 11.13, X_loss = 1.13, Y_loss = 1.63\n",
      "[INFO] Batch #181 | Epoch #1, G_loss = 10.00, F_loss = 9.79, X_loss = 1.25, Y_loss = 1.51\n",
      "[INFO] Batch #182 | Epoch #1, G_loss = 11.04, F_loss = 10.78, X_loss = 1.39, Y_loss = 1.63\n",
      "[INFO] Batch #183 | Epoch #1, G_loss = 9.55, F_loss = 9.31, X_loss = 1.56, Y_loss = 1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Batch #184 | Epoch #1, G_loss = 12.27, F_loss = 12.46, X_loss = 1.27, Y_loss = 1.45\n",
      "[INFO] Batch #185 | Epoch #1, G_loss = 12.57, F_loss = 11.97, X_loss = 1.34, Y_loss = 1.42\n",
      "[INFO] Batch #186 | Epoch #1, G_loss = 9.82, F_loss = 9.84, X_loss = 1.63, Y_loss = 1.51\n",
      "[INFO] Batch #187 | Epoch #1, G_loss = 11.12, F_loss = 11.29, X_loss = 1.46, Y_loss = 1.46\n",
      "[INFO] Batch #188 | Epoch #1, G_loss = 12.30, F_loss = 11.51, X_loss = 1.04, Y_loss = 1.30\n",
      "[INFO] Batch #189 | Epoch #1, G_loss = 11.18, F_loss = 10.40, X_loss = 1.53, Y_loss = 1.43\n",
      "[INFO] Batch #190 | Epoch #1, G_loss = 12.22, F_loss = 11.79, X_loss = 1.31, Y_loss = 1.35\n",
      "[INFO] Batch #191 | Epoch #1, G_loss = 9.92, F_loss = 8.89, X_loss = 1.42, Y_loss = 1.40\n",
      "[INFO] Batch #192 | Epoch #1, G_loss = 9.91, F_loss = 10.12, X_loss = 1.57, Y_loss = 1.44\n",
      "[INFO] Batch #193 | Epoch #1, G_loss = 8.61, F_loss = 8.60, X_loss = 1.27, Y_loss = 1.52\n",
      "[INFO] Batch #194 | Epoch #1, G_loss = 8.30, F_loss = 9.11, X_loss = 1.53, Y_loss = 1.47\n",
      "[INFO] Batch #195 | Epoch #1, G_loss = 7.08, F_loss = 7.15, X_loss = 1.45, Y_loss = 1.54\n",
      "[INFO] Batch #196 | Epoch #1, G_loss = 14.80, F_loss = 13.76, X_loss = 1.23, Y_loss = 1.49\n",
      "[INFO] Batch #197 | Epoch #1, G_loss = 10.20, F_loss = 9.34, X_loss = 1.43, Y_loss = 1.45\n",
      "[INFO] Batch #198 | Epoch #1, G_loss = 14.13, F_loss = 13.94, X_loss = 1.15, Y_loss = 1.50\n",
      "[INFO] Batch #199 | Epoch #1, G_loss = 8.58, F_loss = 8.63, X_loss = 1.44, Y_loss = 1.43\n",
      "[INFO] Batch #200 | Epoch #1, G_loss = 12.00, F_loss = 12.12, X_loss = 1.10, Y_loss = 1.36\n",
      "[INFO] Batch #201 | Epoch #1, G_loss = 9.33, F_loss = 9.58, X_loss = 1.32, Y_loss = 1.36\n",
      "[INFO] Batch #202 | Epoch #1, G_loss = 10.62, F_loss = 10.94, X_loss = 1.47, Y_loss = 1.36\n",
      "[INFO] Batch #203 | Epoch #1, G_loss = 12.09, F_loss = 12.82, X_loss = 1.41, Y_loss = 1.39\n",
      "[INFO] Batch #204 | Epoch #1, G_loss = 12.45, F_loss = 12.14, X_loss = 1.29, Y_loss = 1.40\n",
      "[INFO] Batch #205 | Epoch #1, G_loss = 7.33, F_loss = 7.06, X_loss = 1.55, Y_loss = 1.42\n",
      "[INFO] Batch #206 | Epoch #1, G_loss = 10.18, F_loss = 9.84, X_loss = 1.50, Y_loss = 1.50\n",
      "[INFO] Batch #207 | Epoch #1, G_loss = 9.44, F_loss = 10.07, X_loss = 1.49, Y_loss = 1.55\n",
      "[INFO] Batch #208 | Epoch #1, G_loss = 10.94, F_loss = 10.12, X_loss = 1.23, Y_loss = 1.36\n",
      "[INFO] Batch #209 | Epoch #1, G_loss = 10.95, F_loss = 10.59, X_loss = 1.23, Y_loss = 1.43\n",
      "[INFO] Batch #210 | Epoch #1, G_loss = 10.61, F_loss = 9.80, X_loss = 1.31, Y_loss = 1.31\n",
      "[INFO] Batch #211 | Epoch #1, G_loss = 9.51, F_loss = 9.91, X_loss = 1.48, Y_loss = 1.38\n",
      "[INFO] Batch #212 | Epoch #1, G_loss = 7.39, F_loss = 7.32, X_loss = 1.41, Y_loss = 1.51\n",
      "[INFO] Batch #213 | Epoch #1, G_loss = 9.08, F_loss = 8.96, X_loss = 1.22, Y_loss = 1.48\n",
      "[INFO] Batch #214 | Epoch #1, G_loss = 8.86, F_loss = 8.46, X_loss = 1.43, Y_loss = 1.40\n",
      "[INFO] Batch #215 | Epoch #1, G_loss = 13.52, F_loss = 14.73, X_loss = 1.36, Y_loss = 1.38\n",
      "[INFO] Batch #216 | Epoch #1, G_loss = 13.87, F_loss = 13.63, X_loss = 1.41, Y_loss = 1.31\n",
      "[INFO] Batch #217 | Epoch #1, G_loss = 8.04, F_loss = 7.62, X_loss = 1.40, Y_loss = 1.52\n",
      "[INFO] Batch #218 | Epoch #1, G_loss = 8.49, F_loss = 8.11, X_loss = 1.33, Y_loss = 1.35\n",
      "[INFO] Batch #219 | Epoch #1, G_loss = 8.50, F_loss = 8.85, X_loss = 1.39, Y_loss = 1.42\n",
      "[INFO] Batch #220 | Epoch #1, G_loss = 9.66, F_loss = 9.11, X_loss = 1.35, Y_loss = 1.42\n",
      "[INFO] Batch #221 | Epoch #1, G_loss = 9.56, F_loss = 9.70, X_loss = 1.32, Y_loss = 1.43\n",
      "[INFO] Batch #222 | Epoch #1, G_loss = 13.47, F_loss = 12.32, X_loss = 1.51, Y_loss = 1.46\n",
      "[INFO] Batch #223 | Epoch #1, G_loss = 14.45, F_loss = 13.47, X_loss = 1.13, Y_loss = 1.31\n",
      "[INFO] Batch #224 | Epoch #1, G_loss = 12.55, F_loss = 12.28, X_loss = 1.36, Y_loss = 1.32\n",
      "[INFO] Batch #225 | Epoch #1, G_loss = 10.04, F_loss = 9.94, X_loss = 1.35, Y_loss = 1.31\n",
      "[INFO] Batch #226 | Epoch #1, G_loss = 11.59, F_loss = 10.82, X_loss = 1.17, Y_loss = 1.37\n",
      "[INFO] Batch #227 | Epoch #1, G_loss = 9.95, F_loss = 9.44, X_loss = 1.43, Y_loss = 1.45\n",
      "[INFO] Batch #228 | Epoch #1, G_loss = 10.21, F_loss = 9.37, X_loss = 0.94, Y_loss = 1.38\n",
      "[INFO] Batch #229 | Epoch #1, G_loss = 9.31, F_loss = 9.27, X_loss = 1.42, Y_loss = 1.47\n",
      "[INFO] Batch #230 | Epoch #1, G_loss = 7.53, F_loss = 7.54, X_loss = 1.44, Y_loss = 1.37\n",
      "[INFO] Batch #231 | Epoch #1, G_loss = 8.40, F_loss = 8.53, X_loss = 1.35, Y_loss = 1.44\n",
      "[INFO] Batch #232 | Epoch #1, G_loss = 8.87, F_loss = 8.56, X_loss = 1.29, Y_loss = 1.40\n",
      "[INFO] Batch #233 | Epoch #1, G_loss = 14.90, F_loss = 12.66, X_loss = 2.26, Y_loss = 1.26\n",
      "[INFO] Batch #234 | Epoch #1, G_loss = 13.74, F_loss = 13.51, X_loss = 1.11, Y_loss = 1.33\n",
      "[INFO] Batch #235 | Epoch #1, G_loss = 9.88, F_loss = 9.28, X_loss = 1.42, Y_loss = 1.33\n",
      "[INFO] Batch #236 | Epoch #1, G_loss = 9.44, F_loss = 9.21, X_loss = 1.29, Y_loss = 1.36\n",
      "[INFO] Batch #237 | Epoch #1, G_loss = 7.41, F_loss = 7.08, X_loss = 1.32, Y_loss = 1.34\n",
      "[INFO] Batch #238 | Epoch #1, G_loss = 9.17, F_loss = 8.82, X_loss = 1.21, Y_loss = 1.41\n",
      "[INFO] Batch #239 | Epoch #1, G_loss = 12.05, F_loss = 11.36, X_loss = 1.34, Y_loss = 1.23\n",
      "[INFO] Batch #240 | Epoch #1, G_loss = 8.76, F_loss = 8.54, X_loss = 1.34, Y_loss = 1.32\n",
      "[INFO] Batch #241 | Epoch #1, G_loss = 13.79, F_loss = 13.65, X_loss = 1.23, Y_loss = 1.28\n",
      "[INFO] Batch #242 | Epoch #1, G_loss = 11.76, F_loss = 11.04, X_loss = 1.34, Y_loss = 1.26\n",
      "[INFO] Batch #243 | Epoch #1, G_loss = 11.54, F_loss = 11.47, X_loss = 1.44, Y_loss = 1.20\n",
      "[INFO] Batch #244 | Epoch #1, G_loss = 7.79, F_loss = 7.68, X_loss = 1.45, Y_loss = 1.44\n",
      "[INFO] Batch #245 | Epoch #1, G_loss = 6.90, F_loss = 6.64, X_loss = 1.47, Y_loss = 1.32\n",
      "[INFO] Batch #246 | Epoch #1, G_loss = 9.79, F_loss = 9.65, X_loss = 1.23, Y_loss = 1.39\n",
      "[INFO] Batch #247 | Epoch #1, G_loss = 10.75, F_loss = 10.10, X_loss = 1.09, Y_loss = 1.26\n",
      "[INFO] Batch #248 | Epoch #1, G_loss = 14.22, F_loss = 12.41, X_loss = 1.46, Y_loss = 1.37\n",
      "[INFO] Batch #249 | Epoch #1, G_loss = 10.60, F_loss = 10.12, X_loss = 1.16, Y_loss = 1.58\n",
      "[INFO] Batch #250 | Epoch #1, G_loss = 10.34, F_loss = 10.15, X_loss = 0.93, Y_loss = 1.63\n",
      "[INFO] Batch #251 | Epoch #1, G_loss = 11.28, F_loss = 11.35, X_loss = 1.12, Y_loss = 1.54\n",
      "[INFO] Batch #252 | Epoch #1, G_loss = 7.92, F_loss = 8.33, X_loss = 1.35, Y_loss = 1.55\n",
      "[INFO] Batch #253 | Epoch #1, G_loss = 10.49, F_loss = 9.80, X_loss = 1.51, Y_loss = 1.38\n",
      "[INFO] Batch #254 | Epoch #1, G_loss = 10.48, F_loss = 9.63, X_loss = 1.32, Y_loss = 1.47\n",
      "[INFO] Batch #255 | Epoch #1, G_loss = 9.47, F_loss = 10.27, X_loss = 1.55, Y_loss = 1.60\n",
      "[INFO] Batch #256 | Epoch #1, G_loss = 14.17, F_loss = 14.37, X_loss = 0.84, Y_loss = 1.42\n",
      "[INFO] Batch #257 | Epoch #1, G_loss = 12.36, F_loss = 12.64, X_loss = 1.19, Y_loss = 1.36\n",
      "[INFO] Batch #258 | Epoch #1, G_loss = 9.63, F_loss = 8.41, X_loss = 1.85, Y_loss = 1.34\n",
      "[INFO] Batch #259 | Epoch #1, G_loss = 9.95, F_loss = 9.71, X_loss = 1.59, Y_loss = 1.32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-312b6944403a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                   % (batch_id, i+1 , g_loss.numpy(), f_loss.numpy(), x_loss.numpy(), y_loss.numpy()))\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_horses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_zebras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-312b6944403a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(datasetX, datasetY)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasetY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mbatch_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             print('[INFO] Batch #%d | Epoch #%d, G_loss = %.2f, F_loss = %.2f, X_loss = %.2f, Y_loss = %.2f'\n\u001b[1;32m     66\u001b[0m                   % (batch_id, i+1 , g_loss.numpy(), f_loss.numpy(), x_loss.numpy(), y_loss.numpy()))\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Preparing optimizers ###\n",
    "g_opt = Adam(lr=lr, beta_1=0.5, amsgrad=True)\n",
    "f_opt = Adam(lr=lr, beta_1=0.5, amsgrad=True)\n",
    "x_opt = Adam(lr=lr, beta_1=0.5, amsgrad=True)\n",
    "y_opt = Adam(lr=lr, beta_1=0.5, amsgrad=True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, Y):\n",
    "    ### F : Y -> X, G : X -> Y ###\n",
    "    ### X : separate X and F(Y), Y : separate Y and G(X) ###\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # For binary crossentropy \n",
    "        fake_y = G(X, training=True)\n",
    "        fake_x = F(Y, training=True)\n",
    "        \n",
    "        # For cycle consistency losses \n",
    "        cycled_y = F(fake_y, training=True)\n",
    "        cycled_x = G(fake_x, training=True)\n",
    "        \n",
    "        # For identity losses \n",
    "        same_x   = F(X, training=True)\n",
    "        same_y   = G(Y, training=True)\n",
    "        \n",
    "        D_X_real = D_x(X, training=True)\n",
    "        D_Y_real = D_y(Y, training=True)\n",
    "        D_X_fake = D_x(fake_x, training=True)\n",
    "        D_Y_fake = D_y(fake_y, training=True)\n",
    "        \n",
    "        # Training generators\n",
    "        bce_g = generator_loss(D_Y_fake)\n",
    "        bce_f = generator_loss(D_X_fake)\n",
    "        \n",
    "        total_cycle_loss = cycle_consistency_loss(cycled_x, X) + cycle_consistency_loss(cycled_y, Y)\n",
    "        \n",
    "        identity_g = identity_loss(same_y, Y)\n",
    "        identity_f = identity_loss(same_x, X)\n",
    "        \n",
    "        total_loss_g = bce_g + total_cycle_loss + identity_g\n",
    "        total_loss_f = bce_f + total_cycle_loss + identity_f\n",
    "        \n",
    "        # Training discriminators \n",
    "        total_loss_Dx = discriminator_loss(D_X_real, D_X_fake)\n",
    "        total_loss_Dy = discriminator_loss(D_Y_real, D_Y_fake)\n",
    "        \n",
    "    # Calculate gradients \n",
    "    grad_g = tape.gradient(total_loss_g, G.trainable_variables)\n",
    "    grad_f = tape.gradient(total_loss_f, F.trainable_variables)\n",
    "    grad_Dx = tape.gradient(total_loss_Dx, D_x.trainable_variables)\n",
    "    grad_Dy = tape.gradient(total_loss_Dy, D_y.trainable_variables)\n",
    "    \n",
    "    # Apply gradients on trainable variables\n",
    "    g_opt.apply_gradients(zip(grad_g, G.trainable_variables))\n",
    "    f_opt.apply_gradients(zip(grad_f, F.trainable_variables))\n",
    "    x_opt.apply_gradients(zip(grad_Dx, D_x.trainable_variables))\n",
    "    y_opt.apply_gradients(zip(grad_Dy, D_y.trainable_variables))\n",
    "    \n",
    "    return total_loss_g, total_loss_f, total_loss_Dx, total_loss_Dy\n",
    "    \n",
    "def train(datasetX, datasetY):\n",
    "    for i in range(epochs):\n",
    "        batch_id = 0\n",
    "        for X, Y in tf.data.Dataset.zip((datasetX, datasetY)):\n",
    "            batch_id += 1\n",
    "            g_loss, f_loss, x_loss, y_loss = train_step(X, Y)\n",
    "            print('[INFO] Batch #%d | Epoch #%d, G_loss = %.2f, F_loss = %.2f, X_loss = %.2f, Y_loss = %.2f'\n",
    "                  % (batch_id, i+1 , g_loss.numpy(), f_loss.numpy(), x_loss.numpy(), y_loss.numpy()))\n",
    "            \n",
    "train(train_horses, train_zebras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
